{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 guardrail-ml==0.0.12 tensorboard\n",
    "!apt-get -qq install poppler-utils tesseract-ocr\n",
    "!pip install -q unstructured[\"local-inference\"]==0.7.4 pillow==9.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from guardrail.client import (\n",
    "    run_metrics,\n",
    "    run_simple_metrics,\n",
    "    create_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Used for multi-gpu\n",
    "local_rank = -1\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 1\n",
    "gradient_accumulation_steps = 4\n",
    "learning_rate = 2e-5\n",
    "max_grad_norm = 0.3\n",
    "weight_decay = 0.001\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64\n",
    "max_seq_length = 512\n",
    "\n",
    "# The model that you want to train from the Hugging Face hub\n",
    "model_name = \"guardrail/llama-2-7b-guanaco-instruct-sharded\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"llama-2-7b-guanaco-dolly-your-model\"\n",
    "\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"databricks/databricks-dolly-15k\"\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Activate nested quantization for 4-bit base models\n",
    "use_nested_quant = False\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4=\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16 training\n",
    "fp16 = True\n",
    "\n",
    "# Enable bf16 training\n",
    "bf16 = False\n",
    "\n",
    "# Use packing dataset creating\n",
    "packing = False\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Optimizer to use, original is paged_adamw_32bit\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine, and has advantage for analysis)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of optimizer update steps, 10K original, 20 for demo purposes\n",
    "max_steps = 100\n",
    "\n",
    "# Fraction of steps to do a warmup for\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length (saves memory and speeds up training considerably)\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 10\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 1\n",
    "\n",
    "# The output directory where the model predictions and checkpoints will be written\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# Visualize training\n",
    "report_to = \"tensorboard\"\n",
    "\n",
    "# Tensorboard logs\n",
    "tb_log_dir = \"./results/logs\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16, you can accelerate training with the argument --bf16\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    inference_mode=False,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules = [\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# This is the fix for fp16 training\n",
    "tokenizer.padding_side = \"right\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Human: {sample['instruction']}\"\n",
    "    context = f\"Here's some context: {sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Assistant\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "# apply prompt template per sample\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "# Shuffle the dataset\n",
    "dataset_shuffled = dataset.shuffle(seed=42)\n",
    "\n",
    "# Select the first 250 rows from the shuffled dataset, comment if you want 15k\n",
    "dataset = dataset_shuffled.select(range(50))\n",
    "\n",
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"AlexanderDoria/novel17_test\", split=\"train\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"dolly_llama_formatted_v2 (1).jsonl\", split=\"train\")\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = dataset.map(template_dataset, remove_columns=list(dataset.features))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_eval = load_dataset(\"AlexanderDoria/novel17_test\", split=\"test\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.model.save_pretrained(output_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir results/logs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def text_gen_eval_wrapper(model, tokenizer, prompt, model_id=1, show_metrics=True, max_length=200):\n",
    "    \"\"\"\n",
    "    A wrapper function for inferencing, evaluating, and logging text generation pipeline.\n",
    "\n",
    "    Parameters:\n",
    "        model (str or object): The model name or the initialized text generation model.\n",
    "        tokenizer (str or object): The tokenizer name or the initialized tokenizer for the model.\n",
    "        prompt (str): The input prompt text for text generation.\n",
    "        model_id (int, optional): An identifier for the model. Defaults to 1.\n",
    "        show_metrics (bool, optional): Whether to calculate and show evaluation metrics.\n",
    "                                       Defaults to True.\n",
    "        max_length (int, optional): The maximum length of the generated text sequence.\n",
    "                                    Defaults to 200.\n",
    "\n",
    "    Returns:\n",
    "        generated_text (str): The generated text by the model.\n",
    "        metrics (dict): Evaluation metrics for the generated text (if show_metrics is True).\n",
    "    \"\"\"\n",
    "    # Suppress Hugging Face pipeline logging\n",
    "    logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "    # Initialize the pipeline\n",
    "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=max_length)\n",
    "\n",
    "    # Generate text using the pipeline\n",
    "    result = pipe(prompt)\n",
    "    generated_text = result[0]['generated_text']\n",
    "\n",
    "    # Find the index of \"### Assistant\" in the generated text\n",
    "    index = generated_text.find(\"### Assistant:\")\n",
    "    if index != -1:\n",
    "        # Extract the substring after \"### Assistant\"\n",
    "        substring_after_assistant = generated_text[index + len(\"### Assistant:\"):].strip()\n",
    "    else:\n",
    "        # If \"### Assistant\" is not found, use the entire generated text\n",
    "        substring_after_assistant = generated_text.strip()\n",
    "\n",
    "    if show_metrics:\n",
    "        # Calculate evaluation metrics\n",
    "        metrics = run_metrics(substring_after_assistant, prompt, model_id)\n",
    "\n",
    "        return substring_after_assistant, metrics\n",
    "    else:\n",
    "        return substring_after_assistant"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Inference and evaluate outputs/prompts\n",
    "prompt = \"### Human: Sophie's parents have three daughters: Amy, Jessy, and what’s the name of the third daughter?\"\n",
    "text_gen_eval_wrapper(model, tokenizer, prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = \"### Human: Why can camels survive for long without water? ### Assistant:\"\n",
    "generated_text = text_gen_eval_wrapper(model, tokenizer, prompt, show_metrics=False, max_length=250)\n",
    "print(generated_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = \"### Human: Based on this paragraph about San Diego, what is the largest city in the state of california: San Diego (Spanish for 'Saint Didacus'; /ˌsæn diˈeɪɡoʊ/ SAN dee-AY-goh, Spanish: [san ˈdjeɣo]) is a city on the Pacific Ocean coast of Southern California located immediately adjacent to the Mexico–United States border. With a 2020 population of 1,386,932, it is the eighth most populous city in the United States and the seat of San Diego County, the fifth most populous county in the United States, with 3,286,069 estimated residents as of 2021. The city is known for its mild year-round Mediterranean climate, natural deep-water harbor, extensive beaches and parks, long association with the United States Navy, and recent emergence as a healthcare and biotechnology development center. San Diego is the second largest city in the state of California after Los Angeles. ### Assistant:\"\n",
    "generated_text, metrics = text_gen_eval_wrapper(model, tokenizer, prompt, show_metrics=True, max_length=300)\n",
    "print(generated_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}